{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q flask flask_cors pyngrok transformers accelerate bitsandbytes shap lime scikit-learn numpy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from flask import Flask, request, jsonify\nfrom flask_cors import CORS\nfrom pyngrok import ngrok\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T19:58:15.774918Z","iopub.execute_input":"2025-11-05T19:58:15.775529Z","iopub.status.idle":"2025-11-05T19:58:25.528369Z","shell.execute_reply.started":"2025-11-05T19:58:15.775503Z","shell.execute_reply":"2025-11-05T19:58:25.527533Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"\"\"\"\nExplainable AI (XAI) utilities for the Press Conference Simulator.\n\nModes:\n- \"semantic\": Fast baseline ‚Äî show which sentences in the speech are most similar to the question.\n- \"shap\":     Token-level attribution over the SPEECH for how much it helps explain the QUESTION.\n- \"attention\":Top tokens by last-layer attention (model must be loaded with output_attentions=True).\n- \"lime\":     Placeholder for classifier models exposing predict_proba (off by default).\n\nNotes:\n- No prompt changes are required. This runs post-generation.\n- For SHAP and attention, pass the same `model` and `tokenizer` used to generate.\n\"\"\"\n\nfrom typing import Dict, Any, List\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Optional deps\ntry:\n    import shap\nexcept Exception:\n    shap = None\ntry:\n    from lime.lime_text import LimeTextExplainer\nexcept Exception:\n    LimeTextExplainer = None\n\n\n# ============== Helpers ==============\n\ndef _split_sentences(text: str) -> List[str]:\n    parts = [p.strip() for p in text.replace(\"!\", \".\").replace(\"?\", \".\").split(\".\")]\n    return [p for p in parts if p]\n\ndef _semantic_explain(speech: str, question: str) -> str:\n    segments = _split_sentences(speech)\n    if not segments:\n        return \"No speech segments to analyze.\"\n    corpus = segments + [question]\n    vec = TfidfVectorizer(stop_words=\"english\")\n    X = vec.fit_transform(corpus)\n    sims = cosine_similarity(X[-1], X[:-1]).ravel()\n    if sims.size == 0:\n        return \"No similarity signal detected.\"\n    order = np.argsort(sims)[::-1][:2]\n    tops = [f\"‚Ä¢ \\\"{segments[i]}\\\" (sim={sims[i]:.2f})\" for i in order if sims[i] > 0]\n    return \"Likely influential speech parts:\\n\" + (\"\\n\".join(tops) if tops else \"No strong matches.\")\n\ndef _avg_logprob_for_target(model, tokenizer, context: str, target: str, device: str = None) -> float:\n    import torch\n    model_device = next(model.parameters()).device\n    if device is None:\n        device = str(model_device)\n    with torch.no_grad():\n        full = context + (\"\\n\" if context and not context.endswith(\"\\n\") else \"\") + target\n        enc = tokenizer(full, return_tensors=\"pt\")\n        input_ids = enc[\"input_ids\"].to(model_device)\n        attn_mask = enc.get(\"attention_mask\", None)\n        if attn_mask is not None:\n            attn_mask = attn_mask.to(model_device)\n        ctx_ids = tokenizer(context, return_tensors=\"pt\")[\"input_ids\"].to(model_device)\n        ctx_len = ctx_ids.shape[1]\n        full_len = input_ids.shape[1]\n        tgt_len = full_len - ctx_len\n        if tgt_len <= 0:\n            return float(\"-inf\")\n        outputs = model(input_ids=input_ids, attention_mask=attn_mask)\n        logits = outputs.logits\n        logprobs = logits.log_softmax(dim=-1)\n        target_token_ids = input_ids[:, ctx_len:full_len]\n        prev_positions = logprobs[:, ctx_len-1:full_len-1, :] if ctx_len > 0 else logprobs[:, :full_len-1, :]\n        seq = min(prev_positions.shape[1], target_token_ids.shape[1])\n        prev_positions = prev_positions[:, -seq:, :]\n        target_token_ids = target_token_ids[:, :seq]\n        tok_logprobs = prev_positions.gather(dim=-1, index=target_token_ids.unsqueeze(-1)).squeeze(-1)\n        return float(tok_logprobs.mean().item())\n\n\n\ndef _shap_explain(speech: str, question: str, model, tokenizer) -> str:\n    if shap is None:\n        return \"SHAP not available. Install `shap`.\"\n    try:\n        masker = shap.maskers.Text(tokenizer=lambda x: x.split())\n\n        def score_fn(texts: List[str]) -> np.ndarray:\n            scores = []\n            for t in texts:\n                try:\n                    val = _avg_logprob_for_target(model, tokenizer, t[:512], question[:256])\n                    scores.append(val)\n                except Exception:\n                    scores.append(-999.0)\n            return np.array(scores, dtype=float)\n\n        explainer = shap.Explainer(score_fn, masker)\n        shap_values = explainer([speech[:512]])  # limit speech length\n\n        if shap_values.values is None or not hasattr(shap_values, \"values\"):\n            return \"SHAP returned no values.\"\n\n        token_importance = np.abs(shap_values.values).mean(axis=0)[0]\n        tokens = shap_values.data[0]\n        order = np.argsort(token_importance)[::-1][:8]\n        top = [f\"{tokens[i]} ({token_importance[i]:.3f})\" for i in order if token_importance[i] > 0]\n        return \"Top influential speech tokens (SHAP): \" + (\", \".join(top) if top else \"no signal\")\n    except Exception as e:\n        return f\"SHAP failed ({type(e).__name__}). Fallback:\\n\" + _semantic_explain(speech, question)\n\n\n\n\ndef _attention_explain(text: str, model, tokenizer) -> str:\n    import torch\n\n    # Force-enable attention outputs\n    if not getattr(model.config, \"output_attentions\", False):\n        model.config.output_attentions = True\n\n    # Encode safely\n    enc = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n    enc = {k: v.to(next(model.parameters()).device) for k, v in enc.items()}\n\n    with torch.no_grad():\n#        outputs = model(**enc, output_attentions=True)\n        outputs = model.forward(**enc, output_attentions=True)\n\n\n    if not hasattr(outputs, \"attentions\") or outputs.attentions is None:\n        return \"Attention not available: model did not return attention tensors.\"\n\n    # Take last-layer attention tensor\n    attentions = outputs.attentions[-1]\n    if attentions is None or len(attentions) == 0:\n        return \"No attention data returned.\"\n\n    attn = attentions[0]  # [heads, seq, seq]\n    mean_attn = attn.mean(0).mean(0).cpu().numpy()  # mean over heads\n    tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"][0])\n    order = np.argsort(mean_attn)[::-1][:8]\n    top = [f\"{tokens[i]} ({mean_attn[i]:.3f})\" for i in order]\n    return \"Top attention-weighted tokens: \" + \", \".join(top)\n\n\n\n\ndef explainability_node(state: Dict[str, Any], model=None, tokenizer=None, mode: str = \"semantic\") -> Dict[str, Any]:\n    speech = state.get(\"speech\", \"\") or \"\"\n    question = state.get(\"generated_question\", \"\") or \"\"\n\n    if not speech and not question:\n        state[\"explanation\"] = \"Insufficient data for explainability.\"\n        return state\n\n    try:\n        # --- Semantic mode ---\n        if mode == \"semantic\":\n            state[\"explanation\"] = _semantic_explain(speech, question)\n\n        # --- SHAP mode ---\n        elif mode == \"shap\":\n            if model is None or tokenizer is None:\n                state[\"explanation\"] = \"SHAP requires model/tokenizer.\"\n            else:\n                state[\"explanation\"] = _shap_explain(speech, question, model, tokenizer)\n\n        # --- Attention mode ---\n        elif mode == \"attention\":\n            if model is None or tokenizer is None:\n                state[\"explanation\"] = \"Attention requires model/tokenizer.\"\n            else:\n                combo = (speech + \"\\n\\nQuestion: \" + question).strip()\n                state[\"explanation\"] = _attention_explain(combo, model, tokenizer)\n\n        # --- LIME mode ---\n        elif mode == \"lime\":\n            if LimeTextExplainer is None:\n                state[\"explanation\"] = \"LIME not available (package missing). Install `lime`.\"\n                return state\n\n            explainer = LimeTextExplainer()\n            try:\n                # LIME needs a predict_proba function; we use a mock one for now\n                exp = explainer.explain_instance(\n                    speech,\n                    lambda texts: np.array([[np.random.rand(), np.random.rand()] for _ in texts]),\n                    num_features=6\n                )\n                state[\"explanation\"] = \"LIME placeholder output: \" + str(exp.as_list()[:3])\n            except Exception as e:\n                state[\"explanation\"] = f\"LIME failed ({type(e).__name__}).\"\n\n        # --- Unknown mode ---\n        else:\n            state[\"explanation\"] = \"Unknown explainability mode.\"\n\n    except Exception as e:\n        state[\"explanation\"] = f\"Explainability error: {type(e).__name__}: {e}\"\n\n    return state\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T19:58:32.911123Z","iopub.execute_input":"2025-11-05T19:58:32.911592Z","iopub.status.idle":"2025-11-05T19:58:35.790058Z","shell.execute_reply.started":"2025-11-05T19:58:32.911570Z","shell.execute_reply":"2025-11-05T19:58:35.789216Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# --- 3Ô∏è‚É£ Authenticate ngrok ---\nNGROK_AUTH_TOKEN = \"34ktbqghT9LZGQRy9kuxtWT3P29_6iusbqrquTMMu12cxwwVQ\"   # Replace with your token\n!ngrok config add-authtoken {NGROK_AUTH_TOKEN}\n\n# --- 4Ô∏è‚É£ Initialize Flask app ---\napp = Flask(__name__)\nCORS(app)\n\n# --- 5Ô∏è‚É£ Load model once (GPU recommended) ---\nmodel_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n    device_map=\"auto\" if device == \"cuda\" else None,\n    output_attentions=True\n)\n\n# --- 6Ô∏è‚É£ Define endpoints ---\n@app.route('/explain', methods=['POST'])\ndef explain():\n    data = request.get_json(force=True)\n    speech = data.get(\"speech\", \"\")\n    question = data.get(\"question\", \"\")\n    mode = data.get(\"mode\", \"shap\")\n    print(\"=\"*80)\n    print(f\"üß© Explainability request (mode={mode})\")\n    state = {\"speech\": speech, \"generated_question\": question}\n    explained = explainability_node(state, model=model, tokenizer=tokenizer, mode=mode)\n    print(\"‚úÖ Explanation:\", explained[\"explanation\"])\n    return jsonify({\"explanation\": explained[\"explanation\"]})\n\n@app.route('/generate', methods=['POST'])\ndef generate():\n    data = request.get_json(force=True)\n    prompt = data.get(\"prompt\", \"\")\n    print(\"=\"*80)\n    print(\"üì© Received JSON payload:\", data)\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are an investigative journalist conducting a live interview.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False)\n    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n    outputs = model.generate(**inputs, max_new_tokens=128, temperature=0.8)\n    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(\"‚úÖ Model output:\", response_text)\n    return jsonify({\"response\": response_text.strip()})\n\n@app.route('/ping', methods=['GET'])\ndef ping():\n    return jsonify({\"status\": \"alive\"})\n\n# --- 8Ô∏è‚É£ Create public URL & run ---\npublic_url = ngrok.connect(5000)\nprint(\"‚úÖ Public URL:\", public_url.public_url)\nprint(\"Use this URL in your local Flask app.\")\n\napp.run(port=5000)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T19:58:46.152945Z","iopub.execute_input":"2025-11-05T19:58:46.153955Z","iopub.status.idle":"2025-11-05T20:06:19.451931Z","shell.execute_reply.started":"2025-11-05T19:58:46.153904Z","shell.execute_reply":"2025-11-05T20:06:19.450985Z"}},"outputs":[{"name":"stdout","text":"Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c6ea4284a2d4204b27001c28e07b8fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b59c20bd8494e45be6977e9cc7854ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2467a749e5cf4d2da4f40577fc340a58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38c9a17a1c32419e8094dd260aad0ebd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e36f399558e3468abd309e5e8003b706"}},"metadata":{}},{"name":"stderr","text":"The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\nThe 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n2025-11-05 19:58:58.046147: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762372738.230537      96 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762372738.282855      96 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71d19a0c9c1348728c1fc3ad837339df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b31bcbecbcb414c93906a6b367cd0f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"231e70756c0f4c8283ac66a556173818"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b8e854890cc4740a3460349c15bb4b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcdd39d9d77440858a5d73eff19ff143"}},"metadata":{}},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['output_attentions']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['output_attentions']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3d22fb186d84929b1fb7d4e392755a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11cec48714bc4e8e81daa0c094fa4a8d"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Public URL: https://unbevelled-articularly-linn.ngrok-free.dev\nUse this URL in your local Flask app.\n * Serving Flask app '__main__'\n * Debug mode: off\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"================================================================================\nüì© Received JSON payload: {'prompt': '### ROLE PROMPTING\\n    You are **Investigative Hawk**, a professional journalist.\\n    Your characterization: A relentless fact-checker who probes claims, demands concrete evidence, and exposes inconsistencies without hesitation.\\n    \\n    ### CONTEXT\\n    Press conference topic: **AI in healthcare**\\n    Guest type: **CEO**\\n    Opening statement:\\n    \"\"\"hello we will launch our new ai model for cancer classification , this will detect the desease before two months of it \\n\"\"\"\\n    \\n    Ongoing dialogue transcript:\\n    \\n\\n\\n    \\n    ### CONSTRAINTS\\n    - Ask **only one** concise, sharp, and contextually relevant question.\\n    - Stay aligned with your persona‚Äôs tone.\\n    - Do **not** answer on behalf of the guest.\\n    - Avoid repetition or generic questions.\\n    - Base your question strictly on the dialogue and factual context above.\\n    \\n    ### THINKING (internal, invisible)\\n    Before producing your question, briefly reason about:\\n    1. What new, valuable angle has not been covered?\\n    2. What factual or ethical gap can be exposed?\\n    3. How to phrase the question to elicit meaningful detail?\\n    Then output **only** the final question.\\n    \\n    ### OUTPUT\\n    Produce the next question:'}\n","output_type":"stream"},{"name":"stderr","text":"`sdpa` attention does not support `output_attentions=True` or `head_mask`. Please set your attention to `eager` if you want any of these features.\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Model output: You are an investigative journalist conducting a live interview.\n\n### ROLE PROMPTING\n    You are **Investigative Hawk**, a professional journalist.\n    Your characterization: A relentless fact-checker who probes claims, demands concrete evidence, and exposes inconsistencies without hesitation.\n    \n    ### CONTEXT\n    Press conference topic: **AI in healthcare**\n    Guest type: **CEO**\n    Opening statement:\n    \"\"\"hello we will launch our new ai model for cancer classification , this will detect the desease before two months of it \n\"\"\"\n    \n    Ongoing dialogue transcript:\n    \n\n\n    \n    ### CONSTRAINTS\n    - Ask **only one** concise, sharp, and contextually relevant question.\n    - Stay aligned with your persona‚Äôs tone.\n    - Do **not** answer on behalf of the guest.\n    - Avoid repetition or generic questions.\n    - Base your question strictly on the dialogue and factual context above.\n    \n    ### THINKING (internal, invisible)\n    Before producing your question, briefly reason about:\n    1. What new, valuable angle has not been covered?\n    2. What factual or ethical gap can be exposed?\n    3. How to phrase the question to elicit meaningful detail?\n    Then output **only** the final question.\n    \n    ### OUTPUT\n    Produce the next question: \"Could you elaborate on the scientific basis for your claim that your AI model can detect cancer two months earlier than current methods? What specific data or research supports this assertion, and how does it compare to existing diagnostic tools in terms of accuracy and false positives?\"\n================================================================================\nüß© Explainability request (mode=semantic)\n‚úÖ Explanation: Likely influential speech parts:\n‚Ä¢ \"hello we will launch our new ai model for cancer classification , this will detect the desease before two months of it\" (sim=0.18)\n================================================================================\nüß© Explainability request (mode=attention)\n‚úÖ Explanation: No attention data returned.\n================================================================================\nüß© Explainability request (mode=shap)\n‚úÖ Explanation: SHAP failed (TypeError). Fallback:\nLikely influential speech parts:\n‚Ä¢ \"hello we will launch our new ai model for cancer classification , this will detect the desease before two months of it\" (sim=0.18)\n================================================================================\nüß© Explainability request (mode=lime)\n‚úÖ Explanation: LIME placeholder output: [('model', 0.029863337515340007), ('we', -0.021197488394566717), ('desease', -0.019026803879879157)]\n","output_type":"stream"}],"execution_count":3}]}